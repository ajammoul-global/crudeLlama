# Fake News Detector: Fine-tuned Llama-3.2 with LoRA

## ğŸ“‹ Project Overview

**Fake News Detector** is a production-ready fine-tuned LLaMA 3.2 (3B) model for detecting fake news articles, trained with LoRA (Low-Rank Adaptation) and optimized for deployment on **Kaggle** with automatic upload to **Hugging Face Hub**.

### Key Features
- âœ… **Train on Kaggle**: Automatic environment detection + secret management
- âœ… **Auto-upload to Hub**: Model automatically pushes to HF after training
- âœ… **Model Merging**: Creates both LoRA (~5MB) and merged (~6GB) versions
- âœ… **Two Deployment Options**: Load merged model or base + LoRA adapters
- âœ… **Tokenizer Fallback**: Handles edge cases with TokenizersBackend
- âœ… **Production Ready**: Works on Kaggle, Colab, local, or cloud

### Core Technologies
- **Base Model**: `meta-llama/Llama-3.2-1B` (lightweight, 1B parameters)
- **Fine-tuning**: PEFT LoRA (Low-Rank Adaptation)
- **Quantization**: 4-bit (BitsAndBytes) for GPU memory optimization
- **Framework**: PyTorch + Transformers + PEFT
- **Deployment**: Hugging Face Hub (automatic upload)
- **Dataset**: Fake News Dataset (200 fake + 200 real articles)
- **Training**: Optimized for Kaggle 32GB GPU with gradient accumulation

---

## ğŸ“ Project Structure

```
crudeLlama/
â”œâ”€â”€ config/                    # Configuration classes (paths, hyperparams, model config)
â”‚   â”œâ”€â”€ __init__.py           # Exports ModelConfig, DataConfig, PathConfig, TrainingConfig
â”‚   â”œâ”€â”€ model.py              # Model configuration (quantization, LoRA, tokenizer)
â”‚   â”œâ”€â”€ data.py               # Dataset sampling and preprocessing config
â”‚   â”œâ”€â”€ training.py           # Training hyperparameters
â”‚   â”œâ”€â”€ path.py               # File paths (CSVs, output dirs)
â”‚   â””â”€â”€ login.py              # [Likely for HF token or credentials]
â”‚
â”œâ”€â”€ src/                       # Main source code (modular components)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/                 # Data loading and preprocessing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loader.py         # DataLoader class (loads CSV, samples, splits)
â”‚   â”‚   â””â”€â”€ preprocess.py     # DataPreprocessor (tokenization, formatting)
â”‚   â”‚
â”‚   â”œâ”€â”€ model/                # Model initialization and LoRA
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model.py          # ModelLoader (load base, fine-tuned, tokenizer)
â”‚   â”‚   â””â”€â”€ lora.py           # LoRAManager (apply LoRA config to model)
â”‚   â”‚
â”‚   â”œâ”€â”€ inference/            # Prediction on new articles
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ FakeNewsPredictor.py   # FakeNewsPredictor class (reuses ModelLoader)
â”‚   â”‚
â”‚   â”œâ”€â”€ tunning/              # Training loop and callbacks
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ tune.py           # ModelTrainer (HF Trainer wrapper)
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                # Utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logger.py         # Logging helpers (print_section, print_step)
â”‚   â”‚   â””â”€â”€ memory.py         # GPU memory utilities (clear_memory, print_memory_stats)
â”‚   â”‚
â”‚   â””â”€â”€ export.py             # [Export model to ONNX or other formats]
â”‚
â”œâ”€â”€ data/                      # Datasets
â”‚   â”œâ”€â”€ raw/                  # Raw CSV files
â”‚   â”‚   â”œâ”€â”€ Fake.csv          # ~40K fake news articles
â”‚   â”‚   â””â”€â”€ True.csv          # ~20K real news articles
â”‚   â””â”€â”€ processed/            # [Processed datasets, intermediate files]
â”‚
â”œâ”€â”€ models/                    # Model checkpoints and fine-tuned models
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â””â”€â”€ llama-3.2-1b/    # Base model weights (downloaded from HF)
â”‚   â””â”€â”€ fine-tunned/          # Fine-tuned model directory
â”‚       â”œâ”€â”€ fake_news_detector/  # Main fine-tuned model
â”‚       â”‚   â”œâ”€â”€ adapter_config.json
â”‚       â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚       â”‚   â”œâ”€â”€ tokenizer.json
â”‚       â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚       â”‚   â”œâ”€â”€ README.md
â”‚       â”‚   â””â”€â”€ checkpoint-160/   # Last checkpoint
â”‚       â””â”€â”€ checkpoints/      # Training checkpoints
â”‚
â”œâ”€â”€ testing/                   # Test and validation scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test.py              # Quick accuracy test on sample articles
â”‚   â”œâ”€â”€ test_fine_tunned.py  # [Test fine-tuned model]
â”‚   â””â”€â”€ test_raw.py          # [Test raw/base model]
â”‚
â”œâ”€â”€ checkpoint-160/          # Last training checkpoint (temporary)
â”œâ”€â”€ fake-news-detector-1b/   # [Exported model checkpoint]
â”‚
â”œâ”€â”€ Main.py                  # Main training script (orchestrates pipeline)
â”œâ”€â”€ run.py                   # Inference script (single article prediction)
â”œâ”€â”€ hello.py                 # [Test/demo script]
â”œâ”€â”€ login.py                 # [HF login or credentials]
â”‚
â”œâ”€â”€ Requirements.txt         # Python dependencies
â”œâ”€â”€ Dockerfile              # Docker containerization
â”œâ”€â”€ compose.yaml            # Docker Compose config
â”œâ”€â”€ README.md              # High-level project README
â”œâ”€â”€ README.Docker.md       # Docker setup instructions
â””â”€â”€ env                    # [Environment variables or venv]
```

---

## ğŸ”§ Configuration System

### `config/__init__.py` â€” Central Import Point
Exports all configuration classes:
```python
from config.model import ModelConfig
from config.data import DataConfig
from config.training import TrainingConfig
from config.path import PathConfig
```

### `config/model.py` â€” Model & Tokenizer Configuration
**Purpose**: Define model architecture, quantization, LoRA, and tokenizer settings.

```python
class ModelConfig:
    MODEL_NAME = "meta-llama/Llama-3.2-1B"  # Lightweight 1B model
    LOAD_IN_4BIT = True
    QUANT_TYPE = "nf4"
    COMPUTE_DTYPE = "float16"
    USE_DOUBLE_QUANT = True
    
    LORA_R = 8
    LORA_ALPHA = 16
    LORA_TARGET_MODULES = ["q_proj", "v_proj"]
    LORA_DROPOUT = 0.05
    
    PAD_TOKEN = "eos"
    PADDING_SIDE = "right"
    MAX_SEQ_LENGTH = 256
```

**Key Settings**:
- `LOAD_IN_4BIT`: Quantize model to 4-bit (fits in ~1GB GPU memory vs ~6GB)
- `LORA_R=8`: Rank of LoRA adapters (low-rank approximation)
- `MAX_SEQ_LENGTH=256`: Truncate inputs to 256 tokens
- **Model Size**: 1B parameters (vs 3B previously) for faster training and inference

### `config/data.py` â€” Dataset Configuration
**Purpose**: Control data sampling, preprocessing, and labels.

```python
class DataConfig:
    SAMPLE_SIZE = 200          # 200 fake + 200 real
    TEST_SIZE = 0.2            # 80% train, 20% test
    RANDOM_SEED = 42
    MAX_TITLE_LENGTH = 80
    LABEL_MAP = {0: "Fake", 1: "Real"}
```

### `config/training.py` â€” Training Hyperparameters
**Purpose**: Control training loop (epochs, batch size, learning rate, etc.).

```python
class TrainingConfig:
    NUM_EPOCHS = 2
    BATCH_SIZE_TRAIN = 1
    GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size = 1 * 4 = 4
    LEARNING_RATE = 2e-4
    OPTIMIZER = "paged_adamw_8bit"   # 8-bit optimizer for memory efficiency
    SAVE_STEPS = 100
    EVAL_STEPS = 100
```

**Important Notes**:
- Batch size = 1 + gradient accumulation steps = 4 â†’ effective batch size of 4
- `paged_adamw_8bit` offloads optimizer state to CPU to save GPU memory

### `config/path.py` â€” File Paths & Kaggle/Hub Configuration
**Purpose**: Centralize all file paths (data, models, outputs) + configure HF Hub integration + detect Kaggle environment.

```python
import os

# Auto-detect Kaggle environment
IS_KAGGLE = os.path.exists('/kaggle/working')

# Paths (auto-adjust for Kaggle)
if IS_KAGGLE:
    BASE_DIR = '/kaggle/working'
    DATA_DIR = '/kaggle/input/fake-news-dataset'  # Dataset uploaded to Kaggle
else:
    BASE_DIR = r"C:\Users\lenovo\Desktop\crudeLlama"
    DATA_DIR = os.path.join(BASE_DIR, "data/raw")

FAKE_CSV = os.path.join(DATA_DIR, "Fake.csv")
TRUE_CSV = os.path.join(DATA_DIR, "True.csv")
OUTPUT_DIR = os.path.join(BASE_DIR, "models/fine-tunned/fake_news_detector")
CHECKPOINT_DIR = os.path.join(BASE_DIR, "models/fine-tunned/checkpoints")

# Hugging Face Hub Configuration
HF_TOKEN = os.getenv("HF_TOKEN", "")  # Set in Kaggle secrets
HF_REPO_ID = "your-username/fake-news-detector"  # Update with your username
PUSH_TO_HUB = bool(HF_TOKEN)  # Auto-enable if token exists

# Kaggle Auto-Setup
if IS_KAGGLE and HF_TOKEN:
    from huggingface_hub import login
    login(token=HF_TOKEN)
```

**Key Features**:
- ğŸŸ  **IS_KAGGLE**: Auto-detects Kaggle environment (checks `/kaggle/working`)
- ğŸ“ **Path auto-adjustment**: Uses `/kaggle/working` on Kaggle, local paths otherwise
- ğŸ”‘ **HF_TOKEN**: Reads from Kaggle secrets (won't print in output)
- ğŸš€ **PUSH_TO_HUB**: Auto-enables when token exists (no manual config needed)
- ğŸ“¤ **Auto-login**: Logs into HF Hub on Kaggle if token present

**Setup on Kaggle**:
1. Create a Kaggle secret named `HF_TOKEN` with your HF token value
2. Code automatically detects it and logs in
3. Model uploads to Hub after training completes

---

## ğŸš€ Core Components

### 1. Data Loading (`src/data/loader.py`)

**Class**: `DataLoader`

**Purpose**: Load CSV files, sample data, and create train/test split.

**Key Methods**:
- `load_data()` â†’ Returns HF `datasets.DatasetDict` with 'train' and 'test' splits

**Workflow**:
1. Read `Fake.csv` and `True.csv` from raw data
2. Sample 200 articles from each (controlled by `DataConfig.SAMPLE_SIZE`)
3. Label fake=0, real=1
4. Shuffle and concatenate
5. Split: 80% train, 20% test

**Example Usage**:
```python
loader = DataLoader()
dataset = loader.load_data()  # {'train': Dataset, 'test': Dataset}
print(f"Train: {len(dataset['train'])}, Test: {len(dataset['test'])}")
```

---

### 2. Data Preprocessing (`src/data/preprocess.py`)

**Class**: `DataPreprocessor`

**Purpose**: Tokenize dataset, format prompts, and prepare for training.

**Key Methods**:
- `tokenize_dataset(dataset)` â†’ Returns tokenized dataset with input_ids, attention_mask

**Tokenization Strategy**:
- Truncate to `MAX_SEQ_LENGTH` (256 tokens)
- Pad to fixed length
- Create labels (same as input_ids for causal LM)

**Example Usage**:
```python
preprocessor = DataPreprocessor(tokenizer)
tokenized_data = preprocessor.tokenize_dataset(dataset)  # HF DatasetDict
# Returns: {'train': Dataset(...), 'test': Dataset(...)}
```

---

### 3. Model Loading (`src/model/model.py`)

**Class**: `ModelLoader`

**Purpose**: Unified loader for base model, fine-tuned model, and tokenizer. **Key design: reusable across training and inference.**

**Key Methods**:

#### `load_base_model()`
- Loads `meta-llama/Llama-3.2-3B` with 4-bit quantization
- Used during training
- Returns quantized model

#### `load_finetuned_model(model_path)`
- Loads fine-tuned model with LoRA adapters from local directory
- Used during inference with LoRA adapters
- Expects: `adapter_model.safetensors` + tokenizer files at `model_path`
- Returns: Merged model with LoRA weights

#### `load_tokenizer(path=None)` â­ **WITH FALLBACK**
- Primary: Tries `AutoTokenizer.from_pretrained(path)`
- **Fallback**: If "TokenizersBackend" error, loads raw tokenizer from `tokenizer.json` using `tokenizers` library
- Sets special tokens (pad, eos) from `tokenizer_config.json`
- Returns: `PreTrainedTokenizer` or `PreTrainedTokenizerFast`

#### `merge_and_save_model(model, tokenizer, adapter_path, output_path)` â­ **NEW**
- Merges LoRA adapters with base model into a single complete model
- Used after training to create deployment-ready single-file model
- Steps:
  1. Load base model (unquantized)
  2. Load LoRA adapters from `adapter_path`
  3. Call `merge_and_unload()` to merge weights
  4. Save complete merged model to `output_path`
- Returns: Merged model (ready for inference)

**Tokenizer Loading Flow**:
```
AutoTokenizer.from_pretrained(path)
    â”œâ”€ Success â†’ Return tokenizer
    â””â”€ TokenizersBackend error
       â””â”€ Fallback: tokenizers.Tokenizer.from_file() + wrap with PreTrainedTokenizerFast
```

**Example Usage**:
```python
loader = ModelLoader()

# For training
model = loader.load_base_model()      # Quantized 4-bit
tokenizer = loader.load_tokenizer()   # Default: base model

# After training: merge LoRA with base
merged_model = loader.merge_and_save_model(
    model=model,
    tokenizer=tokenizer,
    adapter_path="./models/fine-tunned/fake_news_detector",
    output_path="./models/fine-tunned/fake_news_detector_merged"
)

# For inference with merged model
tokenizer = loader.load_tokenizer(merged_model_path)
model = AutoModelForCausalLM.from_pretrained(merged_model_path)
```

---

### 4. LoRA Setup (`src/model/lora.py`)

**Class**: `LoRAManager`

**Purpose**: Apply LoRA (Low-Rank Adaptation) adapters to frozen base model.

**Key Methods**:
- `apply_lora(model)` â†’ Returns model with LoRA adapters attached
- `print_trainable_params(model)` â†’ Shows % of trainable params

**LoRA Configuration**:
- **Rank (r)**: 8 (low-rank matrices decomposition)
- **Alpha**: 16 (scaling factor)
- **Target Modules**: `["q_proj", "v_proj"]` (query and value projections in attention)
- **Dropout**: 0.05

**Why LoRA?**
- Only 0.5-5% of parameters are trainable
- Reduces memory usage by ~80%
- Faster training than full fine-tuning

---

### 5. Model Training (`src/tunning/tune.py`)

**Class**: `ModelTrainer`

**Purpose**: Wrapper around HuggingFace `Trainer` for the training loop.

**Key Methods**:
- `train(train_dataset, eval_dataset)` â†’ Returns `Trainer` object after training

**Training Configuration** (from `TrainingConfig`):
- 2 epochs
- Batch size: 1 (gradient accumulation: 4 â†’ effective: 4)
- Learning rate: 2e-4
- Optimizer: `paged_adamw_8bit` (8-bit, CPU-offloaded)
- FP16 precision: Enabled (mixed precision training)
- Gradient checkpointing: Enabled (save memory)
- Eval every 100 steps
- Save every 100 steps

---

### 6. Inference â€” Single Prediction (`src/inference/FakeNewsPredictor.py`)

**Class**: `FakeNewsPredictor`

**Purpose**: Make predictions on new articles. **Reuses `ModelLoader` for consistency.** **Supports both merged model and LoRA adapters.**

**Key Methods**:
- `__init__(model_path=None, use_merged=True)` â†’ Loads merged model by default, falls back to LoRA
- `predict(title, text="")` â†’ Returns dict with prediction, label, confidence
- `predict_batch(articles)` â†’ Predict multiple articles
- `predict_csv(input_csv, output_csv)` â†’ Batch predict from CSV file
- `_load_merged_model()` â†’ Load fully merged model (fast, single file)
- `_load_lora_model()` â†’ Load base + LoRA adapters (flexible, small size)

**Prediction Pipeline**:
1. Format input: `"Classify: {title}\nAnswer:"`
2. Tokenize with max_length=256
3. Generate with `max_new_tokens=10, temperature=0.1`
4. Decode response
5. Extract label by checking if "Real" or "Fake" appears in output
6. Return dict with prediction, label, confidence

**Output Format**:
```python
{
    'prediction': 1,           # 1=Real, 0=Fake, None=Unknown
    'label': 'Real',
    'confidence': 0.9,
    'title': 'Article title',
    'raw_output': 'Classify: Article...\nAnswer: Real'
}
```

**Example Usage**:
```python
# Load merged model (default, fast)
predictor = FakeNewsPredictor()
result = predictor.predict("Scientists discover aliens", "No text provided")
print(f"Prediction: {result['label']} ({result['confidence']:.0%})")

# Load with LoRA adapters (if merged not available)
predictor = FakeNewsPredictor(use_merged=False)

# Batch predict
articles = [
    {'title': 'Article 1', 'text': 'Some text'},
    {'title': 'Article 2', 'text': ''}
]
results = predictor.predict_batch(articles)

# Batch predict from CSV
predictor.predict_csv("input.csv", "output.csv")
```

---

## ğŸ“Š Workflow: Training & Inference

### Training Pipeline (`Main.py`)

```
1. âœ… Detect Environment (Kaggle vs Local)
   â”œâ”€ Display: ğŸŸ  KAGGLE or ğŸ’» LOCAL
   â””â”€ Auto-adjust paths, enable GPU memory optimizations for Kaggle

2. Load base model (quantized 4-bit)
   â””â”€ ModelLoader.load_base_model()

3. Apply LoRA adapters
   â””â”€ LoRAManager.apply_lora(model)

4. Load dataset
   â””â”€ DataLoader.load_data()  â†’ train & test splits

5. Preprocess & tokenize
   â””â”€ DataPreprocessor.tokenize_dataset()

6. Train with HF Trainer
   â””â”€ ModelTrainer.train(train_data, test_data)
       - 2 epochs
       - Save checkpoints every 100 steps
       - Eval every 100 steps
       - ğŸŸ  Kaggle: Optimized batch size & gradient accumulation

7. Save LoRA adapters locally
   â””â”€ model.save_pretrained(OUTPUT_DIR)

8. â­ Merge LoRA with base model
   â””â”€ ModelLoader.merge_and_save_model()
       - Load base model
       - Load LoRA adapters
       - Merge weights into single model
       - Save as complete model

9. â­ Auto-upload to Hugging Face Hub (if token configured)
   â””â”€ Merged model: username/fake-news-detector
   â””â”€ LoRA adapters: username/fake-news-detector-lora
   â””â”€ All tokenizer files + README
```

**Kaggle Automatic Features**:
- ğŸ” Auto-detects Kaggle environment from `/kaggle/working`
- ğŸ”‘ Auto-logs into HF Hub using Kaggle secret `HF_TOKEN`
- ğŸ“¤ Auto-pushes model after training completes
- âœ… Shows environment & Hub config at startup
- ğŸ“Š Displays training progress with GPU stats

**Output**:
```
============================================================
Environment Detected: ğŸŸ  KAGGLE
Hub Configuration: ENABLED
  - Repo: username/fake-news-detector
  - Token: â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢
============================================================
...training...
âœ… Training complete!
âœ… Model merged successfully
âœ… Models uploaded to Hub:
   - Merged: https://huggingface.co/username/fake-news-detector
   - LoRA: https://huggingface.co/username/fake-news-detector-lora
============================================================
```

**Files Created During Training**:

*LoRA Adapters Only (lightweight, ~5MB)*:
- `models/fine-tunned/fake_news_detector/adapter_model.safetensors`
- `models/fine-tunned/fake_news_detector/tokenizer.json`
- `models/fine-tunned/fake_news_detector/tokenizer_config.json`

*Merged Model (complete, ~6GB)*:
- `models/fine-tunned/fake_news_detector_merged/pytorch_model.bin` (or .safetensors)
- `models/fine-tunned/fake_news_detector_merged/config.json`
- `models/fine-tunned/fake_news_detector_merged/tokenizer.json`
- All other config files

*Checkpoints (periodic saves during training)*:
- `models/fine-tunned/checkpoints/checkpoint-XXX/`

### Inference Pipeline (`run.py` or `FakeNewsPredictor`)

```
1. Initialize predictor
   â”œâ”€ Try: Load merged model (fast, single file)
   â”‚  â””â”€ FakeNewsPredictor(use_merged=True)
   â””â”€ Fallback: Load base + LoRA adapters (if merged unavailable)
      â””â”€ FakeNewsPredictor(use_merged=False)

2. Format & tokenize input
   â””â”€ "Classify: {title}\nAnswer:"

3. Generate prediction
   â””â”€ model.generate(max_new_tokens=10)

4. Extract label from output
   â””â”€ Check for "Real" or "Fake"

5. Return result
   â””â”€ {'prediction': 1, 'label': 'Real', 'confidence': 0.9, ...}
```

**Two Deployment Options**:

| Aspect | Merged Model | LoRA Adapters |
|--------|--------------|---------------|
| **File size** | ~6GB (complete) | ~5MB (adapters only) |
| **Inference speed** | Fast (single load) | Slower (load base + adapters) |
| **Flexibility** | Fixed to one config | Can swap adapters |
| **Setup** | Copy one directory | Need base model + adapters |
| **Use case** | Production deployment | Research, multi-model serving |

---

## ğŸ“ Entry Points & Usage

### ğŸŸ  **NEW**: Training on Kaggle with Automatic Hub Upload

**Why Kaggle?**
- âœ… Free 32GB GPU (T4 or P100)
- âœ… No setup required (libraries pre-installed)
- âœ… Auto-logout on finish (no hanging processes)
- âœ… Built-in notebook environment

**Quick Setup**:
1. Go to https://www.kaggle.com/settings/account
2. Create a new notebook
3. Copy code from `KAGGLE_QUICK_START.md`
4. Add your HF token as a Kaggle secret (name it `HF_TOKEN`)
5. Run the notebook!

**What Happens Automatically**:
- Detects Kaggle environment
- Reads your HF token from Kaggle secrets
- Trains the model
- Merges LoRA with base model
- **Uploads everything to your HF Hub account**
- Shows Hub URLs at finish

**See These Guides**:
- ğŸ“„ `KAGGLE_QUICK_START.md` - Start here (5 minutes)
- ğŸ“„ `KAGGLE_SETUP.md` - Detailed walkthrough
- ğŸ“„ `KAGGLE_CHECKLIST.md` - Pre-training checklist
- ğŸ“„ `KAGGLE_RESOURCES.md` - Navigation guide

---

### 1. Training: `Main.py` (Local or Kaggle)

**Local Training**:
```bash
python Main.py
```

**Kaggle Training** (Recommended):
- Use `KAGGLE_QUICK_START.md` for copy-paste notebook code
- Add `HF_TOKEN` as Kaggle secret
- Run and auto-upload to Hub!

**What it does**: 
- ğŸ” Auto-detects Kaggle vs local environment
- ğŸ“¥ Loads base model + applies LoRA
- ğŸ“Š Loads data + preprocesses
- ğŸ”„ Trains for 2 epochs
- ğŸ’¾ Saves LoRA adapters locally
- ğŸ”— **Merges LoRA with base model**
- ğŸ“¤ **Auto-uploads to Hugging Face Hub** (if token configured)
- ğŸŸ  **Kaggle-specific**: Auto-detects secrets, optimized batch sizing

**Output**:
- Local: `models/fine-tunned/fake_news_detector/` (LoRA) + `fake_news_detector_merged/` (merged)
- Hub (automatic on Kaggle):
  - `username/fake-news-detector` (merged model)
  - `username/fake-news-detector-lora` (LoRA adapters)
- Both are accessible from any notebook: Kaggle, Colab, local

---

### 2. Single Prediction: `run.py`
```bash
python run.py "Article title" "Optional article text"
```
**Example**:
```bash
python run.py "Scientists discover aliens on Mars"
```
**What it does**:
- Loads merged model from local disk (or Hub if configured)
- Predicts on input article
- Prints prediction + confidence

**Output**:
```
Loading model...
âœ“ Predictor ready!
============================================================
PREDICTION RESULT
============================================================

Title: Scientists discover aliens on Mars
â†’ Prediction: Fake
â†’ Confidence: 90%
```

**Load from Hub**:
```bash
python -c "
from src.inference.FakeNewsPredictor import FakeNewsPredictor
p = FakeNewsPredictor('your-username/fake-news-detector', from_hub=True)
result = p.predict('Article title')
print(result)
"
```

---

### 3. Quick Test: `testing/test.py`
```bash
python .\testing\test.py
```
**What it does**:
- Auto-detects local fine-tuned model
- Tests on 4 sample articles
- Prints accuracy

**Output**:
```
Loading model from C:\...\fake_news_detector...
âœ“ Model loaded successfully!

Test 1:
  Title: Scientists cure cancer with lemon juice
  True: Fake
  Predicted: Fake
  âœ“

Quick Test Accuracy: 100% (4/4)
```

---

## ğŸŒ Using Models from Hugging Face Hub

### Setup (One-time)

**On Local Machine**:
1. Get HF token: https://huggingface.co/settings/tokens
2. Authenticate: `huggingface-cli login`
3. Edit `config/path.py`:
   ```python
   HF_REPO_ID = "your-username/fake-news-detector"
   PUSH_TO_HUB = True
   ```
4. Run `Main.py` - models upload automatically after training

**On Kaggle** (Recommended):
1. Add HF token as Kaggle secret (name: `HF_TOKEN`)
2. Code auto-detects it and uploads (no config needed!)
3. See `KAGGLE_QUICK_START.md` for copy-paste notebook code

### After Training
Models automatically uploaded to:
- **Merged**: `https://huggingface.co/your-username/fake-news-detector` (~6GB, complete model)
- **LoRA**: `https://huggingface.co/your-username/fake-news-detector-lora` (~5MB, adapters only)

### Load from Hub (Any Environment)
```python
from src.inference.FakeNewsPredictor import FakeNewsPredictor

# Load merged model from Hub
predictor = FakeNewsPredictor(
    model_path="your-username/fake-news-detector",
    from_hub=True,
    use_merged=True
)

result = predictor.predict("Article title")
print(f"Prediction: {result['label']}")
```

### Deploy in Production
```python
# Load once, reuse for multiple predictions
predictor = FakeNewsPredictor("your-username/fake-news-detector", from_hub=True)

# Serve predictions via API or app
results = predictor.predict_batch([
    {"title": "Article 1"},
    {"title": "Article 2"}
])
```

### Full HF Hub Guide
See `HF_HUB_GUIDE.md` for detailed setup, troubleshooting, and advanced usage.

### Full Kaggle Integration Guide
See `KAGGLE_RESOURCES.md` for navigation to all Kaggle-specific guides (KAGGLE_QUICK_START.md, KAGGLE_SETUP.md, etc.)

---

## âš™ï¸ Key Technical Details

### Memory Optimization Strategies

1. **4-bit Quantization** (`ModelConfig.LOAD_IN_4BIT`)
   - Reduces model size from 3B params â†’ ~800MB (instead of 6GB+)
   - Minimal accuracy loss

2. **LoRA Adapters** (`LoRAManager`)
   - Only train 0.5% of parameters
   - Freeze base model weights
   - Save only adapter_model.safetensors (~1.5MB)

3. **Gradient Checkpointing** (`TrainingConfig.GRADIENT_CHECKPOINTING`)
   - Recompute activations during backward pass instead of storing
   - Trade compute for memory

4. **8-bit Optimizer** (`paged_adamw_8bit`)
   - Offload optimizer states to CPU
   - Further reduce GPU memory usage

5. **Small Batch Size** (`BATCH_SIZE_TRAIN=1` + `GRADIENT_ACCUMULATION_STEPS=4`)
   - Effective batch=4 with tiny per-step memory footprint

### Tokenizer Fallback Mechanism

**Problem**: HF's `AutoTokenizer.from_pretrained()` fails if tokenizer_config.json references "TokenizersBackend" class that isn't registered.

**Solution** (in `ModelLoader.load_tokenizer()`):
```
Try AutoTokenizer â†’ Fail with TokenizersBackend error
    â†“
Catch ValueError
    â†“
Load raw tokenizer: tokenizers.Tokenizer.from_file("tokenizer.json")
    â†“
Wrap: PreTrainedTokenizerFast(tokenizer_object=tk)
    â†“
Set special tokens from tokenizer_config.json
    â†“
Success âœ“
```

---

## ğŸ“¦ Dependencies

### Core Libraries
- **torch**: Deep learning framework
- **transformers**: HF Transformers (AutoTokenizer, AutoModel, etc.)
- **peft**: Parameter-Efficient Fine-Tuning (LoRA)
- **bitsandbytes**: 4-bit quantization
- **datasets**: HF datasets library (for dataset management)
- **pandas**: Data manipulation

### Optional
- **tensorboard**: Logging during training
- **accelerate**: Distributed training support
- **tokenizers**: Fast tokenizers (fallback loader)

### Installation
```bash
pip install -r Requirements.txt
```

---

## ğŸ› Troubleshooting

### 1. TokenizersBackend Error
**Error**: `ValueError: Tokenizer class TokenizersBackend does not exist or is not currently imported.`

**Solution**: Already handled! The fallback mechanism in `ModelLoader.load_tokenizer()` catches this and loads from `tokenizer.json` directly.

**If it still fails**:
- Ensure `models/fine-tunned/fake_news_detector/tokenizer.json` exists
- Verify `tokenizers` package is installed: `pip install tokenizers`

---

### 2. Out of Memory (OOM)
**Error**: `CUDA out of memory. Tried to allocate X.XXGiB`

**Solutions**:
- Reduce `BATCH_SIZE_TRAIN` in `config/training.py`
- Increase `GRADIENT_ACCUMULATION_STEPS`
- Enable gradient checkpointing (already enabled by default)
- Reduce `MAX_SEQ_LENGTH` in `config/model.py`

---

### 3. Model Not Found
**Error**: `No local fine-tuned model directory found.`

**Solution**: Ensure trained model exists at `models/fine-tunned/fake_news_detector/` with:
- `adapter_model.safetensors`
- `tokenizer.json`
- `tokenizer_config.json`

Run `python Main.py` to train if missing.

---

### 4. Inference Accuracy Issues
If predictions are consistently wrong:
- Verify model was trained (check checkpoint age)
- Try different temperature: `self.model.generate(..., temperature=0.0)` for deterministic
- Check prompt format matches training prompt
- Manually inspect raw_output in prediction result

---

## ğŸ” Code Reusability & Design Patterns

### Pattern 1: Configuration Centralization
All settings in `config/` module. Easy to modify without touching source code.

### Pattern 2: ModelLoader Reuse
Same `ModelLoader` class used in both training (`Main.py`) and inference (`FakeNewsPredictor`). Ensures consistency.

### Pattern 3: Modular Pipelines
- Data: `DataLoader` â†’ `DataPreprocessor`
- Model: `ModelLoader` â†’ `LoRAManager` â†’ `ModelTrainer`
- Inference: `FakeNewsPredictor` (reuses ModelLoader)

### Pattern 4: Graceful Fallbacks
Tokenizer loading has a safe fallback mechanism for edge cases.

---

## ğŸ“š Next Steps for Development

### Short-term Enhancements
1. Add model evaluation metrics (precision, recall, F1)
2. Implement confidence threshold filtering
3. Add batch CSV prediction with progress bar
4. Create model quantization export (ONNX, TFLite)

### Medium-term Features
1. Multi-class classification (not just Fake/Real)
2. Explainability: show which tokens contributed to prediction
3. A/B testing different LoRA configurations
4. Real-time API endpoint (FastAPI/Flask)

### Long-term
1. Larger base models (7B, 13B LLaMA versions)
2. Ensemble predictions
3. Domain-specific fine-tuning (e.g., medical vs political fake news)
4. Continuous retraining pipeline

---

## ğŸ“ Support & Contact

For questions or issues:
1. Check logs in `models/fine-tunned/fake_news_detector/trainer_state.json`
2. Review training loss curves in checkpoints
3. Inspect `raw_output` from predictions for debugging
4. Verify path configuration in `config/path.py`

---

## ğŸ¯ Quick Links

| Task | File/Link |
|------|-----------|
| **Train on Kaggle** | `KAGGLE_QUICK_START.md` ğŸŸ  START HERE |
| **Detailed Kaggle Setup** | `KAGGLE_SETUP.md` |
| **Pre-training Checklist** | `KAGGLE_CHECKLIST.md` |
| **All Kaggle Guides** | `KAGGLE_RESOURCES.md` |
| **Hub Integration Details** | `HF_HUB_GUIDE.md` |
| **Model Merging Details** | `MERGE_IMPLEMENTATION.md` |
| **Train Locally** | `python Main.py` |
| **Make Predictions** | `python run.py "Article title"` |
| **Quick Test** | `python testing/test.py` |

---

**Last Updated**: December 26, 2025
**Base Model**: meta-llama/Llama-3.2-1B (1B parameters, lightweight)
**Framework**: PyTorch + HF Transformers + PEFT + BitsAndBytes
**Key Feature**: ğŸŸ  Automatic Kaggle + Hub integration (no manual config needed!)
