# Crude Llama: Fake News Detection with LLaMA 3.2

## ğŸ“‹ Project Overview

**Crude Llama** is a fine-tuned LLaMA 3.2 (3B) model specifically trained for fake news detection using QLoRA (Quantized Low-Rank Adaptation). The project includes:
- **Training pipeline**: Fine-tune base model on fake news dataset
- **Inference pipeline**: Make predictions on new articles
- **Modular architecture**: Reusable components for model loading, data processing, and prediction

### Key Technologies
- **Base Model**: `meta-llama/Llama-3.2-3B`
- **Fine-tuning**: PEFT (LoRA adapters)
- **Quantization**: 4-bit quantization via BitsAndBytes
- **Framework**: PyTorch + Transformers + PEFT
- **Dataset**: Fake News Dataset (Kaggle)

---

## ğŸ“ Project Structure

```
crudeLlama/
â”œâ”€â”€ config/                    # Configuration classes (paths, hyperparams, model config)
â”‚   â”œâ”€â”€ __init__.py           # Exports ModelConfig, DataConfig, PathConfig, TrainingConfig
â”‚   â”œâ”€â”€ model.py              # Model configuration (quantization, LoRA, tokenizer)
â”‚   â”œâ”€â”€ data.py               # Dataset sampling and preprocessing config
â”‚   â”œâ”€â”€ training.py           # Training hyperparameters
â”‚   â”œâ”€â”€ path.py               # File paths (CSVs, output dirs)
â”‚   â””â”€â”€ login.py              # [Likely for HF token or credentials]
â”‚
â”œâ”€â”€ src/                       # Main source code (modular components)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/                 # Data loading and preprocessing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loader.py         # DataLoader class (loads CSV, samples, splits)
â”‚   â”‚   â””â”€â”€ preprocess.py     # DataPreprocessor (tokenization, formatting)
â”‚   â”‚
â”‚   â”œâ”€â”€ model/                # Model initialization and LoRA
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model.py          # ModelLoader (load base, fine-tuned, tokenizer)
â”‚   â”‚   â””â”€â”€ lora.py           # LoRAManager (apply LoRA config to model)
â”‚   â”‚
â”‚   â”œâ”€â”€ inference/            # Prediction on new articles
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ FakeNewsPredictor.py   # FakeNewsPredictor class (reuses ModelLoader)
â”‚   â”‚
â”‚   â”œâ”€â”€ tunning/              # Training loop and callbacks
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ tune.py           # ModelTrainer (HF Trainer wrapper)
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                # Utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logger.py         # Logging helpers (print_section, print_step)
â”‚   â”‚   â””â”€â”€ memory.py         # GPU memory utilities (clear_memory, print_memory_stats)
â”‚   â”‚
â”‚   â””â”€â”€ export.py             # [Export model to ONNX or other formats]
â”‚
â”œâ”€â”€ data/                      # Datasets
â”‚   â”œâ”€â”€ raw/                  # Raw CSV files
â”‚   â”‚   â”œâ”€â”€ Fake.csv          # ~40K fake news articles
â”‚   â”‚   â””â”€â”€ True.csv          # ~20K real news articles
â”‚   â””â”€â”€ processed/            # [Processed datasets, intermediate files]
â”‚
â”œâ”€â”€ models/                    # Model checkpoints and fine-tuned models
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â””â”€â”€ llama-3.2-1b/    # Base model weights (downloaded from HF)
â”‚   â””â”€â”€ fine-tunned/          # Fine-tuned model directory
â”‚       â”œâ”€â”€ fake_news_detector/  # Main fine-tuned model
â”‚       â”‚   â”œâ”€â”€ adapter_config.json
â”‚       â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚       â”‚   â”œâ”€â”€ tokenizer.json
â”‚       â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚       â”‚   â”œâ”€â”€ README.md
â”‚       â”‚   â””â”€â”€ checkpoint-160/   # Last checkpoint
â”‚       â””â”€â”€ checkpoints/      # Training checkpoints
â”‚
â”œâ”€â”€ testing/                   # Test and validation scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test.py              # Quick accuracy test on sample articles
â”‚   â”œâ”€â”€ test_fine_tunned.py  # [Test fine-tuned model]
â”‚   â””â”€â”€ test_raw.py          # [Test raw/base model]
â”‚
â”œâ”€â”€ checkpoint-160/          # Last training checkpoint (temporary)
â”œâ”€â”€ fake-news-detector-1b/   # [Exported model checkpoint]
â”‚
â”œâ”€â”€ Main.py                  # Main training script (orchestrates pipeline)
â”œâ”€â”€ run.py                   # Inference script (single article prediction)
â”œâ”€â”€ hello.py                 # [Test/demo script]
â”œâ”€â”€ login.py                 # [HF login or credentials]
â”‚
â”œâ”€â”€ Requirements.txt         # Python dependencies
â”œâ”€â”€ Dockerfile              # Docker containerization
â”œâ”€â”€ compose.yaml            # Docker Compose config
â”œâ”€â”€ README.md              # High-level project README
â”œâ”€â”€ README.Docker.md       # Docker setup instructions
â””â”€â”€ env                    # [Environment variables or venv]
```

---

## ğŸ”§ Configuration System

### `config/__init__.py` â€” Central Import Point
Exports all configuration classes:
```python
from config.model import ModelConfig
from config.data import DataConfig
from config.training import TrainingConfig
from config.path import PathConfig
```

### `config/model.py` â€” Model & Tokenizer Configuration
**Purpose**: Define model architecture, quantization, LoRA, and tokenizer settings.

```python
class ModelConfig:
    MODEL_NAME = "meta-llama/Llama-3.2-3B"
    LOAD_IN_4BIT = True
    QUANT_TYPE = "nf4"
    COMPUTE_DTYPE = "float16"
    USE_DOUBLE_QUANT = True
    
    LORA_R = 8
    LORA_ALPHA = 16
    LORA_TARGET_MODULES = ["q_proj", "v_proj"]
    LORA_DROPOUT = 0.05
    
    PAD_TOKEN = "eos"
    PADDING_SIDE = "right"
    MAX_SEQ_LENGTH = 256
```

**Key Settings**:
- `LOAD_IN_4BIT`: Quantize model to 4-bit to fit on limited GPU memory
- `LORA_R=8`: Rank of LoRA adapters (low-rank approximation)
- `MAX_SEQ_LENGTH=256`: Truncate inputs to 256 tokens

### `config/data.py` â€” Dataset Configuration
**Purpose**: Control data sampling, preprocessing, and labels.

```python
class DataConfig:
    SAMPLE_SIZE = 200          # 200 fake + 200 real
    TEST_SIZE = 0.2            # 80% train, 20% test
    RANDOM_SEED = 42
    MAX_TITLE_LENGTH = 80
    LABEL_MAP = {0: "Fake", 1: "Real"}
```

### `config/training.py` â€” Training Hyperparameters
**Purpose**: Control training loop (epochs, batch size, learning rate, etc.).

```python
class TrainingConfig:
    NUM_EPOCHS = 2
    BATCH_SIZE_TRAIN = 1
    GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size = 1 * 4 = 4
    LEARNING_RATE = 2e-4
    OPTIMIZER = "paged_adamw_8bit"   # 8-bit optimizer for memory efficiency
    SAVE_STEPS = 100
    EVAL_STEPS = 100
```

**Important Notes**:
- Batch size = 1 + gradient accumulation steps = 4 â†’ effective batch size of 4
- `paged_adamw_8bit` offloads optimizer state to CPU to save GPU memory

### `config/path.py` â€” File Paths
**Purpose**: Centralize all file paths (data, models, outputs).

```python
class PathConfig:
    FAKE_CSV = r"C:\Users\lenovo\Desktop\crudeLlama\data\raw\Fake.csv"
    TRUE_CSV = r"C:\Users\lenovo\Desktop\crudeLlama\data\raw\True.csv"
    OUTPUT_DIR = r"C:\Users\lenovo\Desktop\crudeLlama\models\fine-tunned\fake_news_detector"
    CHECKPOINT_DIR = r"C:\Users\lenovo\Desktop\crudeLlama\models\fine-tunned\checkpoints"
```

**Note**: Use raw strings (`r"..."`) for Windows paths to avoid escape sequence issues.

---

## ğŸš€ Core Components

### 1. Data Loading (`src/data/loader.py`)

**Class**: `DataLoader`

**Purpose**: Load CSV files, sample data, and create train/test split.

**Key Methods**:
- `load_data()` â†’ Returns HF `datasets.DatasetDict` with 'train' and 'test' splits

**Workflow**:
1. Read `Fake.csv` and `True.csv` from raw data
2. Sample 200 articles from each (controlled by `DataConfig.SAMPLE_SIZE`)
3. Label fake=0, real=1
4. Shuffle and concatenate
5. Split: 80% train, 20% test

**Example Usage**:
```python
loader = DataLoader()
dataset = loader.load_data()  # {'train': Dataset, 'test': Dataset}
print(f"Train: {len(dataset['train'])}, Test: {len(dataset['test'])}")
```

---

### 2. Data Preprocessing (`src/data/preprocess.py`)

**Class**: `DataPreprocessor`

**Purpose**: Tokenize dataset, format prompts, and prepare for training.

**Key Methods**:
- `tokenize_dataset(dataset)` â†’ Returns tokenized dataset with input_ids, attention_mask

**Tokenization Strategy**:
- Truncate to `MAX_SEQ_LENGTH` (256 tokens)
- Pad to fixed length
- Create labels (same as input_ids for causal LM)

**Example Usage**:
```python
preprocessor = DataPreprocessor(tokenizer)
tokenized_data = preprocessor.tokenize_dataset(dataset)  # HF DatasetDict
# Returns: {'train': Dataset(...), 'test': Dataset(...)}
```

---

### 3. Model Loading (`src/model/model.py`)

**Class**: `ModelLoader`

**Purpose**: Unified loader for base model, fine-tuned model, and tokenizer. **Key design: reusable across training and inference.**

**Key Methods**:

#### `load_base_model()`
- Loads `meta-llama/Llama-3.2-3B` with 4-bit quantization
- Used during training
- Returns quantized model

#### `load_finetuned_model(model_path)`
- Loads fine-tuned model with LoRA adapters from local directory
- Used during inference with LoRA adapters
- Expects: `adapter_model.safetensors` + tokenizer files at `model_path`
- Returns: Merged model with LoRA weights

#### `load_tokenizer(path=None)` â­ **WITH FALLBACK**
- Primary: Tries `AutoTokenizer.from_pretrained(path)`
- **Fallback**: If "TokenizersBackend" error, loads raw tokenizer from `tokenizer.json` using `tokenizers` library
- Sets special tokens (pad, eos) from `tokenizer_config.json`
- Returns: `PreTrainedTokenizer` or `PreTrainedTokenizerFast`

#### `merge_and_save_model(model, tokenizer, adapter_path, output_path)` â­ **NEW**
- Merges LoRA adapters with base model into a single complete model
- Used after training to create deployment-ready single-file model
- Steps:
  1. Load base model (unquantized)
  2. Load LoRA adapters from `adapter_path`
  3. Call `merge_and_unload()` to merge weights
  4. Save complete merged model to `output_path`
- Returns: Merged model (ready for inference)

**Tokenizer Loading Flow**:
```
AutoTokenizer.from_pretrained(path)
    â”œâ”€ Success â†’ Return tokenizer
    â””â”€ TokenizersBackend error
       â””â”€ Fallback: tokenizers.Tokenizer.from_file() + wrap with PreTrainedTokenizerFast
```

**Example Usage**:
```python
loader = ModelLoader()

# For training
model = loader.load_base_model()      # Quantized 4-bit
tokenizer = loader.load_tokenizer()   # Default: base model

# After training: merge LoRA with base
merged_model = loader.merge_and_save_model(
    model=model,
    tokenizer=tokenizer,
    adapter_path="./models/fine-tunned/fake_news_detector",
    output_path="./models/fine-tunned/fake_news_detector_merged"
)

# For inference with merged model
tokenizer = loader.load_tokenizer(merged_model_path)
model = AutoModelForCausalLM.from_pretrained(merged_model_path)
```

---

### 4. LoRA Setup (`src/model/lora.py`)

**Class**: `LoRAManager`

**Purpose**: Apply LoRA (Low-Rank Adaptation) adapters to frozen base model.

**Key Methods**:
- `apply_lora(model)` â†’ Returns model with LoRA adapters attached
- `print_trainable_params(model)` â†’ Shows % of trainable params

**LoRA Configuration**:
- **Rank (r)**: 8 (low-rank matrices decomposition)
- **Alpha**: 16 (scaling factor)
- **Target Modules**: `["q_proj", "v_proj"]` (query and value projections in attention)
- **Dropout**: 0.05

**Why LoRA?**
- Only 0.5-5% of parameters are trainable
- Reduces memory usage by ~80%
- Faster training than full fine-tuning

---

### 5. Model Training (`src/tunning/tune.py`)

**Class**: `ModelTrainer`

**Purpose**: Wrapper around HuggingFace `Trainer` for the training loop.

**Key Methods**:
- `train(train_dataset, eval_dataset)` â†’ Returns `Trainer` object after training

**Training Configuration** (from `TrainingConfig`):
- 2 epochs
- Batch size: 1 (gradient accumulation: 4 â†’ effective: 4)
- Learning rate: 2e-4
- Optimizer: `paged_adamw_8bit` (8-bit, CPU-offloaded)
- FP16 precision: Enabled (mixed precision training)
- Gradient checkpointing: Enabled (save memory)
- Eval every 100 steps
- Save every 100 steps

---

### 6. Inference â€” Single Prediction (`src/inference/FakeNewsPredictor.py`)

**Class**: `FakeNewsPredictor`

**Purpose**: Make predictions on new articles. **Reuses `ModelLoader` for consistency.** **Supports both merged model and LoRA adapters.**

**Key Methods**:
- `__init__(model_path=None, use_merged=True)` â†’ Loads merged model by default, falls back to LoRA
- `predict(title, text="")` â†’ Returns dict with prediction, label, confidence
- `predict_batch(articles)` â†’ Predict multiple articles
- `predict_csv(input_csv, output_csv)` â†’ Batch predict from CSV file
- `_load_merged_model()` â†’ Load fully merged model (fast, single file)
- `_load_lora_model()` â†’ Load base + LoRA adapters (flexible, small size)

**Prediction Pipeline**:
1. Format input: `"Classify: {title}\nAnswer:"`
2. Tokenize with max_length=256
3. Generate with `max_new_tokens=10, temperature=0.1`
4. Decode response
5. Extract label by checking if "Real" or "Fake" appears in output
6. Return dict with prediction, label, confidence

**Output Format**:
```python
{
    'prediction': 1,           # 1=Real, 0=Fake, None=Unknown
    'label': 'Real',
    'confidence': 0.9,
    'title': 'Article title',
    'raw_output': 'Classify: Article...\nAnswer: Real'
}
```

**Example Usage**:
```python
# Load merged model (default, fast)
predictor = FakeNewsPredictor()
result = predictor.predict("Scientists discover aliens", "No text provided")
print(f"Prediction: {result['label']} ({result['confidence']:.0%})")

# Load with LoRA adapters (if merged not available)
predictor = FakeNewsPredictor(use_merged=False)

# Batch predict
articles = [
    {'title': 'Article 1', 'text': 'Some text'},
    {'title': 'Article 2', 'text': ''}
]
results = predictor.predict_batch(articles)

# Batch predict from CSV
predictor.predict_csv("input.csv", "output.csv")
```

---

## ğŸ“Š Workflow: Training & Inference

### Training Pipeline (`Main.py`)

```
1. Load base model (quantized 4-bit)
   â””â”€ ModelLoader.load_base_model()

2. Apply LoRA adapters
   â””â”€ LoRAManager.apply_lora(model)

3. Load dataset
   â””â”€ DataLoader.load_data()  â†’ train & test splits

4. Preprocess & tokenize
   â””â”€ DataPreprocessor.tokenize_dataset()

5. Train with HF Trainer
   â””â”€ ModelTrainer.train(train_data, test_data)
       - 2 epochs
       - Save checkpoints every 100 steps
       - Eval every 100 steps

6. Save LoRA adapters
   â””â”€ model.save_pretrained(PathConfig.MODEL_OUTPUT_DIR)
       tokenizer.save_pretrained(PathConfig.MODEL_OUTPUT_DIR)

7. Merge LoRA with base model
   â””â”€ ModelLoader.merge_and_save_model()
       - Load base model
       - Load LoRA adapters
       - Merge weights
       - Save as complete model
```

**Files Created During Training**:

*LoRA Adapters Only (lightweight, ~5MB)*:
- `models/fine-tunned/fake_news_detector/adapter_model.safetensors`
- `models/fine-tunned/fake_news_detector/tokenizer.json`
- `models/fine-tunned/fake_news_detector/tokenizer_config.json`

*Merged Model (complete, ~6GB)*:
- `models/fine-tunned/fake_news_detector_merged/pytorch_model.bin` (or .safetensors)
- `models/fine-tunned/fake_news_detector_merged/config.json`
- `models/fine-tunned/fake_news_detector_merged/tokenizer.json`
- All other config files

*Checkpoints (periodic saves during training)*:
- `models/fine-tunned/checkpoints/checkpoint-XXX/`

### Inference Pipeline (`run.py` or `FakeNewsPredictor`)

```
1. Initialize predictor
   â”œâ”€ Try: Load merged model (fast, single file)
   â”‚  â””â”€ FakeNewsPredictor(use_merged=True)
   â””â”€ Fallback: Load base + LoRA adapters (if merged unavailable)
      â””â”€ FakeNewsPredictor(use_merged=False)

2. Format & tokenize input
   â””â”€ "Classify: {title}\nAnswer:"

3. Generate prediction
   â””â”€ model.generate(max_new_tokens=10)

4. Extract label from output
   â””â”€ Check for "Real" or "Fake"

5. Return result
   â””â”€ {'prediction': 1, 'label': 'Real', 'confidence': 0.9, ...}
```

**Two Deployment Options**:

| Aspect | Merged Model | LoRA Adapters |
|--------|--------------|---------------|
| **File size** | ~6GB (complete) | ~5MB (adapters only) |
| **Inference speed** | Fast (single load) | Slower (load base + adapters) |
| **Flexibility** | Fixed to one config | Can swap adapters |
| **Setup** | Copy one directory | Need base model + adapters |
| **Use case** | Production deployment | Research, multi-model serving |

---

## ğŸ“ Entry Points & Usage

### 1. Training: `Main.py`
```bash
python Main.py
```
**What it does**: 
- Loads base model + applies LoRA
- Loads data + preprocesses
- Trains for 2 epochs
- Saves fine-tuned model to `models/fine-tunned/fake_news_detector/`

---

### 2. Single Prediction: `run.py`
```bash
python run.py "Article title" "Optional article text"
```
**Example**:
```bash
python run.py "Scientists discover aliens on Mars"
```
**Output**:
```
Loading model...
âœ“ Predictor ready!
============================================================
PREDICTION RESULT
============================================================

Title: Scientists discover aliens on Mars
â†’ Prediction: Fake
â†’ Confidence: 90%
```

---

### 3. Quick Test: `testing/test.py`
```bash
python .\testing\test.py
```
**What it does**:
- Auto-detects local fine-tuned model
- Tests on 4 sample articles
- Prints accuracy

**Output**:
```
Loading model from C:\...\fake_news_detector...
âœ“ Model loaded successfully!

Test 1:
  Title: Scientists cure cancer with lemon juice
  True: Fake
  Predicted: Fake
  âœ“

Quick Test Accuracy: 100% (4/4)
```

---

## âš™ï¸ Key Technical Details

### Memory Optimization Strategies

1. **4-bit Quantization** (`ModelConfig.LOAD_IN_4BIT`)
   - Reduces model size from 3B params â†’ ~800MB (instead of 6GB+)
   - Minimal accuracy loss

2. **LoRA Adapters** (`LoRAManager`)
   - Only train 0.5% of parameters
   - Freeze base model weights
   - Save only adapter_model.safetensors (~1.5MB)

3. **Gradient Checkpointing** (`TrainingConfig.GRADIENT_CHECKPOINTING`)
   - Recompute activations during backward pass instead of storing
   - Trade compute for memory

4. **8-bit Optimizer** (`paged_adamw_8bit`)
   - Offload optimizer states to CPU
   - Further reduce GPU memory usage

5. **Small Batch Size** (`BATCH_SIZE_TRAIN=1` + `GRADIENT_ACCUMULATION_STEPS=4`)
   - Effective batch=4 with tiny per-step memory footprint

### Tokenizer Fallback Mechanism

**Problem**: HF's `AutoTokenizer.from_pretrained()` fails if tokenizer_config.json references "TokenizersBackend" class that isn't registered.

**Solution** (in `ModelLoader.load_tokenizer()`):
```
Try AutoTokenizer â†’ Fail with TokenizersBackend error
    â†“
Catch ValueError
    â†“
Load raw tokenizer: tokenizers.Tokenizer.from_file("tokenizer.json")
    â†“
Wrap: PreTrainedTokenizerFast(tokenizer_object=tk)
    â†“
Set special tokens from tokenizer_config.json
    â†“
Success âœ“
```

---

## ğŸ“¦ Dependencies

### Core Libraries
- **torch**: Deep learning framework
- **transformers**: HF Transformers (AutoTokenizer, AutoModel, etc.)
- **peft**: Parameter-Efficient Fine-Tuning (LoRA)
- **bitsandbytes**: 4-bit quantization
- **datasets**: HF datasets library (for dataset management)
- **pandas**: Data manipulation

### Optional
- **tensorboard**: Logging during training
- **accelerate**: Distributed training support
- **tokenizers**: Fast tokenizers (fallback loader)

### Installation
```bash
pip install -r Requirements.txt
```

---

## ğŸ› Troubleshooting

### 1. TokenizersBackend Error
**Error**: `ValueError: Tokenizer class TokenizersBackend does not exist or is not currently imported.`

**Solution**: Already handled! The fallback mechanism in `ModelLoader.load_tokenizer()` catches this and loads from `tokenizer.json` directly.

**If it still fails**:
- Ensure `models/fine-tunned/fake_news_detector/tokenizer.json` exists
- Verify `tokenizers` package is installed: `pip install tokenizers`

---

### 2. Out of Memory (OOM)
**Error**: `CUDA out of memory. Tried to allocate X.XXGiB`

**Solutions**:
- Reduce `BATCH_SIZE_TRAIN` in `config/training.py`
- Increase `GRADIENT_ACCUMULATION_STEPS`
- Enable gradient checkpointing (already enabled by default)
- Reduce `MAX_SEQ_LENGTH` in `config/model.py`

---

### 3. Model Not Found
**Error**: `No local fine-tuned model directory found.`

**Solution**: Ensure trained model exists at `models/fine-tunned/fake_news_detector/` with:
- `adapter_model.safetensors`
- `tokenizer.json`
- `tokenizer_config.json`

Run `python Main.py` to train if missing.

---

### 4. Inference Accuracy Issues
If predictions are consistently wrong:
- Verify model was trained (check checkpoint age)
- Try different temperature: `self.model.generate(..., temperature=0.0)` for deterministic
- Check prompt format matches training prompt
- Manually inspect raw_output in prediction result

---

## ğŸ” Code Reusability & Design Patterns

### Pattern 1: Configuration Centralization
All settings in `config/` module. Easy to modify without touching source code.

### Pattern 2: ModelLoader Reuse
Same `ModelLoader` class used in both training (`Main.py`) and inference (`FakeNewsPredictor`). Ensures consistency.

### Pattern 3: Modular Pipelines
- Data: `DataLoader` â†’ `DataPreprocessor`
- Model: `ModelLoader` â†’ `LoRAManager` â†’ `ModelTrainer`
- Inference: `FakeNewsPredictor` (reuses ModelLoader)

### Pattern 4: Graceful Fallbacks
Tokenizer loading has a safe fallback mechanism for edge cases.

---

## ğŸ“š Next Steps for Development

### Short-term Enhancements
1. Add model evaluation metrics (precision, recall, F1)
2. Implement confidence threshold filtering
3. Add batch CSV prediction with progress bar
4. Create model quantization export (ONNX, TFLite)

### Medium-term Features
1. Multi-class classification (not just Fake/Real)
2. Explainability: show which tokens contributed to prediction
3. A/B testing different LoRA configurations
4. Real-time API endpoint (FastAPI/Flask)

### Long-term
1. Larger base models (7B, 13B LLaMA versions)
2. Ensemble predictions
3. Domain-specific fine-tuning (e.g., medical vs political fake news)
4. Continuous retraining pipeline

---

## ğŸ“ Support & Contact

For questions or issues:
1. Check logs in `models/fine-tunned/fake_news_detector/trainer_state.json`
2. Review training loss curves in checkpoints
3. Inspect `raw_output` from predictions for debugging
4. Verify path configuration in `config/path.py`

---

**Last Updated**: December 23, 2025
**Base Model**: meta-llama/Llama-3.2-3B
**Framework**: PyTorch + HF Transformers + PEFT
